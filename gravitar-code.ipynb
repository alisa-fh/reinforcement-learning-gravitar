{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dueling Double DQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRkNX7uqYRac"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "rh5NFurCYNJG",
        "outputId": "9d143f7e-9403-4dcb-82c9-5fe0325c16cd"
      },
      "source": [
        "'''\n",
        "This is my Reinforcement Learning submission for playing the notoriously hard\n",
        "Gravitar game, Atari. In all honesty, when familiarising myself with and playing the\n",
        "game myself, I struggled to get a score higher than 200, so I suspected a simple\n",
        "DQN approach would be quite pathetic, and it was time for the big guns,\n",
        "as backed by research papers.  Although relatively old, a relevant paper I found\n",
        "an interesting read is https://arxiv.org/pdf/1710.02298.pdf.\n",
        "\n",
        "I was really determined to go down the policy gradient appraoch initially, and\n",
        "got together an A2C Actor Critic algorithm. The majority of my time was spent trying\n",
        "to implement a Self Imitation Learning (SIL) component to this, from the paper \n",
        "https://arxiv.org/abs/2012.11989. I unfortunately struggled to get it working, \n",
        "and am quite disappointed about that. Because my stubborn nature desperately \n",
        "wanted to get it working, by the time I decided to try a different approach, \n",
        "I was running out of time. So although I flowered up a regular DQN algorithm,\n",
        "my final product isn't as complex as I would have liked, so I apologise.\n",
        "\n",
        "My final implementation made use of https://arxiv.org/abs/1511.06581, the initial\n",
        "RL assignment template, https://github.com/dxyang/DQN_pytorch and \n",
        "https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26.\n",
        "\n",
        "Of course, not expecting marks from this, but if interested in scanning over \n",
        "my A2C SIL attempt, it is here (very messy admittedly): \n",
        "https://colab.research.google.com/drive/1aMrBq2mTJdHaCcUvmENte7HXzaPQ3OtO?usp=sharing\n",
        "\n",
        "Thanks for reading over and marking! This has been super enjoyable and a real challenge.\n",
        "\n",
        "'''"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nThis is my Reinforcement Learning submission for playing the notoriously hard\\nGravitar game, Atari. In all honesty, when familiarising myself with and playing the\\ngame myself, I struggled to get a score higher than 200, so I suspected a simple\\nDQN approach would be quite pathetic, and it was time for the big guns,\\nas backed by research papers.  Although relatively old, a relevant paper I found\\nan interesting read is https://arxiv.org/pdf/1710.02298.pdf.\\n\\nI was really determined to go down the policy gradient appraoch initially, and\\ngot together an A2C Actor Critic algorithm. The majority of my time was spent trying\\nto implement a Self Imitation Learning (SIL) component to this, from the paper \\nhttps://arxiv.org/abs/2012.11989. I unfortunately struggled to get it working, \\nand am quite disappointed about that. Because my stubborn nature desperately \\nwanted to get it working, by the time I decided to try a different approach, \\nI was running out of time. So although I flowered up a regular DQN algorithm,\\nmy final product isn't as complex as I would have liked, so I apologise.\\n\\nMy final implementation made use of https://arxiv.org/abs/1511.06581, the initial\\nRL assignment template, https://github.com/dxyang/DQN_pytorch and \\nhttps://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26.\\n\\nOf course, not expecting marks from this, but if interested in scanning over \\nmy A2C SIL attempt, it is here (very messy admittedly): \\nhttps://colab.research.google.com/drive/1aMrBq2mTJdHaCcUvmENte7HXzaPQ3OtO?usp=sharing\\n\\nThanks for reading over and marking! This has been super enjoyable and a real challenge.\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za0LpZrLYOLV"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D98CddQuwKG"
      },
      "source": [
        "import gym\n",
        "import cv2\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import collections\n",
        "from collections import deque"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8rGMlN2uzgk"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "video_every = 7\n",
        "print_every = 1\n",
        "env = gym.make(\"Gravitar-v0\")\n",
        "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda episode_id: (episode_id%video_every)==0,force=True)\n",
        "seed = 742\n",
        "torch.manual_seed(seed)\n",
        "env.seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "env.action_space.seed(seed)\n",
        "gamma = 0.98\n",
        "lr = 0.00025\n",
        "epsilon = 1\n",
        "eps_decay = 0.9998\n",
        "eps_min = 0.025\n",
        "batch_size = 32\n",
        "buffer_limit = 1000\n",
        "h = env.observation_space.shape[0]\n",
        "w = env.observation_space.shape[1]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5MXmV_YCkrI"
      },
      "source": [
        "def resize_decolourise(image):\n",
        "    image = image[::2, ::2]\n",
        "    image = np.mean(image, axis = 2).astype(np.uint8) // 2\n",
        "    return image\n",
        "def get_tensor(pre_tensor, dtype):\n",
        "    if dtype == \"float\":\n",
        "        return torch.FloatTensor(pre_tensor).to(device)\n",
        "    elif dtype == \"long\":\n",
        "        return torch.LongTensor(pre_tensor).to(device)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTyZZDSYTQc9"
      },
      "source": [
        "class ReplayBuffer():\n",
        "  def __init__(self):\n",
        "    self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "  \n",
        "  def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "  def put(self, s, action, r, s_prime, done):\n",
        "    self.buffer.append([s[None, :], action, r, s_prime[None, :], done])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV3azlqHxzus"
      },
      "source": [
        "# simple block of convolution, batchnorm, and relu\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_f, out_f, in_kernel, in_stride):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.f = nn.Sequential(\n",
        "            nn.Conv2d(in_f, out_f, kernel_size=in_kernel, stride = in_stride),\n",
        "            nn.BatchNorm2d(out_f),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.f(x)\n",
        "        \n",
        "# simple block of convolution, batchnorm, and leakyrelu\n",
        "class LinearBlock(nn.Module):\n",
        "    def __init__(self, in_f, out_f):\n",
        "        super(LinearBlock, self).__init__()\n",
        "        self.f = nn.Sequential(\n",
        "            nn.Linear(in_features=in_f, out_features=128),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Linear( 128, out_f)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.f(x)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxF5-bzUu1q-"
      },
      "source": [
        "class ConvDuelingDQN(nn.Module):\n",
        "    def __init__(self, h, w, output_size):\n",
        "        super(ConvDuelingDQN, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            ConvBlock(4, 32, 8, 4),\n",
        "            ConvBlock(32, 64, 4, 2),\n",
        "            ConvBlock(64, 64, 3, 1)\n",
        "        )\n",
        "\n",
        "        self.advantage = nn.Sequential(\n",
        "            LinearBlock(3456, output_size) #3456 is a result of 6 * 9 *64\n",
        "        )\n",
        "        self.value = nn.Sequential(\n",
        "            LinearBlock(3456, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        state_advantage = self.advantage(x)\n",
        "        state_value = self.value(x)\n",
        "        return state_value + (state_advantage - state_advantage.mean())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plT51MPbu5U5"
      },
      "source": [
        "class ActModel():\n",
        "    def __init__(self):\n",
        "        self.primary_model = ConvDuelingDQN(h=105, w=80, output_size=env.action_space.n).to(device)\n",
        "        self.target_model = ConvDuelingDQN(h=105, w=80, output_size=env.action_space.n).to(device)\n",
        "        self.target_model.load_state_dict(self.primary_model.state_dict())\n",
        "        self.target_model.eval()\n",
        "        self.optimizer = optim.Adam(self.primary_model.parameters(), lr=lr)\n",
        "\n",
        "    def sample_action(self, s):\n",
        "        if random.uniform(0, 1) <= epsilon:\n",
        "            return random.randrange(env.action_space.n)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                s = get_tensor(s, \"float\").unsqueeze(0)\n",
        "                q_values = self.primary_model.forward(s)\n",
        "                return torch.argmax(q_values).item()\n",
        "\n",
        "    def train(self, memory):\n",
        "        s, action, r, s_prime, done = zip(*random.sample(memory.buffer, batch_size))\n",
        "        s = np.concatenate(s)\n",
        "        s_prime = np.concatenate(s_prime)\n",
        "\n",
        "        s = get_tensor(s, \"float\")\n",
        "        s_prime = get_tensor(s_prime, \"float\")\n",
        "        action = get_tensor(action, \"long\")\n",
        "        r = get_tensor(r, \"float\")\n",
        "        done = get_tensor(done, \"float\")\n",
        "\n",
        "        s_q = self.primary_model(s)\n",
        "        s_prime_q = self.primary_model(s_prime)\n",
        "        s_prime_target_q= self.target_model(s_prime)\n",
        "\n",
        "        current_q = s_q.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "        s_prime_target_q = s_prime_target_q.gather(1, s_prime_q.max(1)[1].unsqueeze(1)).squeeze(1)\n",
        "        \n",
        "        #Bellman equation\n",
        "        expected_q = r + gamma * s_prime_target_q * (1 - done)\n",
        "\n",
        "        loss = (current_q - expected_q.detach()).pow(2).mean()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve4vYDe3bozg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9e6c1c8-dff4-43d5-f404-73947b562ea3"
      },
      "source": [
        "actmodel = ActModel()\n",
        "memory = ReplayBuffer()\n",
        "marking = []\n",
        "last_100_r = deque(maxlen=100)\n",
        "cumulative_steps = 1 \n",
        "\n",
        "for n_episode in range(int(1e32)):\n",
        "    s = resize_decolourise(env.reset()) \n",
        "    s = np.stack((s, s, s, s))\n",
        "\n",
        "    r_total = 0\n",
        "    episode_steps = 1\n",
        "    while True:\n",
        "\n",
        "        action = actmodel.sample_action(s)  # Act\n",
        "        s_prime, r, done, _ = env.step(action)  # Observe\n",
        "        s_prime = resize_decolourise(s_prime)  # Process image\n",
        "        s_prime = np.stack((s_prime, s[0], s[1], s[2]))\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.put(s, action, r, s_prime, done)  # Store to mem\n",
        "\n",
        "        # Move to the next state\n",
        "        s = s_prime  # Update state\n",
        "\n",
        "        # Perform one step of the optimization (on the target network)\n",
        "        if memory.size() > 20000:\n",
        "            actmodel.train(memory)\n",
        "\n",
        "        r_total += r\n",
        "        cumulative_steps += 1\n",
        "        episode_steps+=1\n",
        "        if cumulative_steps % 1000 == 0:\n",
        "            epsilon = max(eps_min, epsilon*eps_decay )\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Train model\n",
        "    actmodel.target_model.load_state_dict(actmodel.primary_model.state_dict())  # Update target model\n",
        "    last_100_r.append(r_total)\n",
        "    score = r_total\n",
        "    marking.append(score)\n",
        "\n",
        "    if n_episode%100 == 0:\n",
        "        print(\"marking, episode: {}, score: {:.1f}, mean_score: {:.2f}, std_score: {:.2f}\".format(\n",
        "            n_episode, score, np.array(marking).mean(), np.array(marking).std()))\n",
        "        marking = []\n",
        "\n",
        "    if n_episode%print_every==0:\n",
        "        print(\"Episode:{} Score:{:.2f} Last_100_Avg_Rew:{:.3f} Epsilon:{:.2f} Steps:{}\".format(\n",
        "            n_episode, score, np.mean(last_100_r), epsilon, episode_steps))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "marking, episode: 0, score: 200.0, mean_score: 200.00, std_score: 0.00\n",
            "Episode:0 Score:200.00 Last_100_Avg_Rew:200.000 Epsilon:1.00 Steps:739\n",
            "Episode:1 Score:0.00 Last_100_Avg_Rew:100.000 Epsilon:1.00 Steps:739\n",
            "Episode:2 Score:250.00 Last_100_Avg_Rew:150.000 Epsilon:1.00 Steps:747\n",
            "Episode:3 Score:250.00 Last_100_Avg_Rew:175.000 Epsilon:1.00 Steps:764\n",
            "Episode:4 Score:350.00 Last_100_Avg_Rew:210.000 Epsilon:1.00 Steps:753\n",
            "Episode:5 Score:0.00 Last_100_Avg_Rew:175.000 Epsilon:1.00 Steps:751\n",
            "Episode:6 Score:250.00 Last_100_Avg_Rew:185.714 Epsilon:1.00 Steps:748\n",
            "Episode:7 Score:0.00 Last_100_Avg_Rew:162.500 Epsilon:1.00 Steps:754\n",
            "Episode:8 Score:350.00 Last_100_Avg_Rew:183.333 Epsilon:1.00 Steps:1170\n",
            "Episode:9 Score:250.00 Last_100_Avg_Rew:190.000 Epsilon:1.00 Steps:1233\n",
            "Episode:10 Score:0.00 Last_100_Avg_Rew:172.727 Epsilon:1.00 Steps:753\n",
            "Episode:11 Score:350.00 Last_100_Avg_Rew:187.500 Epsilon:1.00 Steps:821\n",
            "Episode:12 Score:100.00 Last_100_Avg_Rew:180.769 Epsilon:1.00 Steps:790\n",
            "Episode:13 Score:200.00 Last_100_Avg_Rew:182.143 Epsilon:1.00 Steps:809\n",
            "Episode:14 Score:750.00 Last_100_Avg_Rew:220.000 Epsilon:1.00 Steps:668\n",
            "Episode:15 Score:100.00 Last_100_Avg_Rew:212.500 Epsilon:1.00 Steps:719\n",
            "Episode:16 Score:250.00 Last_100_Avg_Rew:214.706 Epsilon:1.00 Steps:1130\n",
            "Episode:17 Score:750.00 Last_100_Avg_Rew:244.444 Epsilon:1.00 Steps:1149\n",
            "Episode:18 Score:850.00 Last_100_Avg_Rew:276.316 Epsilon:1.00 Steps:1124\n",
            "Episode:19 Score:800.00 Last_100_Avg_Rew:302.500 Epsilon:1.00 Steps:817\n",
            "Episode:20 Score:0.00 Last_100_Avg_Rew:288.095 Epsilon:1.00 Steps:844\n",
            "Episode:21 Score:250.00 Last_100_Avg_Rew:286.364 Epsilon:1.00 Steps:736\n",
            "Episode:22 Score:0.00 Last_100_Avg_Rew:273.913 Epsilon:1.00 Steps:992\n",
            "Episode:23 Score:350.00 Last_100_Avg_Rew:277.083 Epsilon:1.00 Steps:874\n",
            "Episode:24 Score:100.00 Last_100_Avg_Rew:270.000 Epsilon:1.00 Steps:793\n",
            "Episode:25 Score:450.00 Last_100_Avg_Rew:276.923 Epsilon:1.00 Steps:790\n",
            "Episode:26 Score:100.00 Last_100_Avg_Rew:270.370 Epsilon:1.00 Steps:743\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}